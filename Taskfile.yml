version: "3"

tasks:
  download-pdfs:
    cmds:
      - curl -o assets/PubLayNet_PDF.tar.gz https://dax-cdn.cdn.appdomain.cloud/dax-publaynet/1.0.0/PubLayNet_PDF.tar.gz
    generates:
      - assets/PubLayNet_PDF.tar.gz
  # these next 3 we don't actually use for anything since we want the PDFs, but they're here
  # for reference to document the URLs.
  download-images: curl -o assets/publaynet.tar.gz https://dax-cdn.cdn.appdomain.cloud/dax-publaynet/1.0.0/publaynet.tar.gz
  download-images-parallel: "parallel --jobs 7 curl -o ./assets/train-{}.tar.gz https://dax-cdn.cdn.appdomain.cloud/dax-publaynet/1.0.0/train-{}.tar.gz ::: {0..6}"
  download-labels: curl -o assets/labels.tar.gz https://dax-cdn.cdn.appdomain.cloud/dax-publaynet/1.0.0/labels.tar.gz
  extract-pdfs:
    cmds:
      - tar -xvzf assets/PubLayNet_PDF.tar.gz -C extracted/
    sources:
      - assets/PubLayNet_PDF.tar.gz
  validate-splits: python scripts/validate_splits.py extracted/pdfs
  merge-pages:
    cmds:
      - python scripts/merge_pages.py extracted/pdfs merged/ data/merged_metadata.csv
    generates:
      - data/merged_metadata.csv
    preconditions:
      # this checks that the correct number of file are in each split
      # if we use 'sources' instead, it has to hash 350k files, which takes too long
      - ls extracted/pdfs/train | wc -l | grep -q 335703
      - ls extracted/pdfs/dev | wc -l | grep -q 11245
      - ls extracted/pdfs/test | wc -l | grep -q 11405
  get-download-urls:
    cmds:
      - python scripts/get_download_urls.py merged/complete data/download_urls.jsonl
    status:
      # this checks that the correct number of URLs are available
      - ls merged/complete | wc -l | grep -q 9131
      - cat data/download_urls.jsonl | wc -l | grep -q 9131

  get-pubmed-files:
    cmds:
      - "python scripts/filter_download_urls.py data/download_urls.jsonl | parallel curl --connect-timeout 10 --output-dir assets/pubmed -C - -O {}"
    sources:
      - data/download_urls.jsonl
    deps: [get-download-urls]
  extract-pubmed-files:
    cmds:
      - ls assets/pubmed | parallel --jobs 8 tar -xvzf assets/pubmed/{} -C extracted/pubmed
      - find extracted/pubmed/ -type f -not -name '*.pdf' | parallel --jobs 8 rm -f {}
    status:
      # Counts PDF files in destination folder, will fail if it's 0
      - find extracted/pubmed/ -type f -name '*.pdf' | wc -l | grep "\b0\b" --invert-match
      # Counts the number of non-PDF files and detects whether it's 0
      - find extracted/pubmed/ -type f -not -name '*.pdf' | wc -l | grep "\b0\b"
  calculate-pdf-statistics:
    cmds:
      - python scripts/calculate_statistics.py
